{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Submission: Rework all the features + ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train shape: (17499636, 19)\n",
      " Test shape: (4393179, 19)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')\n",
    "\n",
    "print(f\" Train shape: {train.shape}\")\n",
    "print(f\" Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New features : more trends, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removed 4271 cancellation events (leakage prevention)\n",
      "Processing 19,140 users...\n",
      " Step 1/10: Basic counting features...\n",
      " Step 2/10: Recency features...\n",
      " Step 3/10: Temporal trend features...\n",
      " Step 4/10: Page-level features...\n",
      " Step 5/10: Downgrade detection...\n",
      " Step 6/10: Session features...\n",
      " Step 7/10: Engagement features...\n",
      " Step 8/10: Time-of-day features...\n",
      " Step 9/10: Rolling window features...\n",
      " Step 10/10: RFM score...\n",
      " Created 56 features for 19,140 users\n",
      "Processing 2,904 users...\n",
      " Step 1/10: Basic counting features...\n",
      " Step 2/10: Recency features...\n",
      " Step 3/10: Temporal trend features...\n",
      " Step 4/10: Page-level features...\n",
      " Step 5/10: Downgrade detection...\n",
      " Step 6/10: Session features...\n",
      " Step 7/10: Engagement features...\n",
      " Step 8/10: Time-of-day features...\n",
      " Step 9/10: Rolling window features...\n",
      " Step 10/10: RFM score...\n",
      " Created 58 features for 2,904 users\n"
     ]
    }
   ],
   "source": [
    "def build_churn_features(df, is_train=True, reference_date='2018-11-20'):\n",
    "\n",
    "    reference_ts = pd.Timestamp(reference_date)\n",
    "\n",
    "    if is_train:\n",
    "        df_features = df[df['page'] != 'Cancellation Confirmation'].copy()\n",
    "        print(f\"  Removed {len(df) - len(df_features)} cancellation events (leakage prevention)\")\n",
    "    else:\n",
    "        df_features = df.copy()\n",
    "\n",
    "    df_features['ts'] = pd.to_datetime(df_features['ts'])\n",
    "    df_features['date'] = df_features['ts'].dt.date\n",
    "    df_features['hour'] = df_features['ts'].dt.hour\n",
    "    df_features['dayofweek'] = df_features['ts'].dt.dayofweek\n",
    "    df_features['is_weekend'] = df_features['dayofweek'].isin([5, 6]).astype(int)\n",
    "    df_features['days_from_start'] = (df_features['ts'] - df_features['ts'].min()).dt.days\n",
    "\n",
    "    df_features['quartile'] = pd.cut(df_features['days_from_start'], bins=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "    user_ids = df_features['userId'].unique()\n",
    "    features = pd.DataFrame({'userId': user_ids})\n",
    "    print(f\"Processing {len(user_ids):,} users...\")\n",
    "\n",
    "    print(\" Step 1/10: Basic counting features...\")\n",
    "    features = features.merge(df_features.groupby('userId').size().rename('total_events'), on='userId', how='left')\n",
    "    total_days = (df_features['ts'].max() - df_features['ts'].min()).days + 1\n",
    "    features['events_per_day'] = features['total_events'] / total_days\n",
    "    features = features.merge(df_features.groupby('userId')['date'].nunique().rename('days_active'), on='userId', how='left')\n",
    "    features['activity_rate'] = features['days_active'] / total_days\n",
    "\n",
    "    print(\" Step 2/10: Recency features...\")\n",
    "    last_activity = df_features.groupby('userId')['ts'].max()\n",
    "    features = features.merge(last_activity.rename('last_ts'), on='userId', how='left')\n",
    "    features['days_since_last_activity'] = (reference_ts - features['last_ts']).dt.days\n",
    "    features['recency_score'] = 1 / (features['days_since_last_activity'] + 1)\n",
    "\n",
    "    last_song = df_features[df_features['page'] == 'NextSong'].groupby('userId')['ts'].max()\n",
    "    features = features.merge(last_song.rename('last_song_ts'), on='userId', how='left')\n",
    "    features['days_since_last_song'] = (reference_ts - features['last_song_ts']).dt.days\n",
    "    features['days_since_last_song'] = features['days_since_last_song'].fillna(999)\n",
    "\n",
    "    print(\" Step 3/10: Temporal trend features...\")\n",
    "    quartile_counts = df_features.groupby(['userId', 'quartile']).size().unstack(fill_value=0)\n",
    "    quartile_counts.columns = [f'activity_{col}' for col in quartile_counts.columns]\n",
    "    features = features.merge(quartile_counts, on='userId', how='left')\n",
    "\n",
    "    for q in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "        if f'activity_{q}' not in features.columns:\n",
    "            features[f'activity_{q}'] = 0\n",
    "\n",
    "    features['decline_ratio'] = (features['activity_Q1'] - features['activity_Q4']) / (features['total_events'] + 1)\n",
    "    features['early_late_ratio'] = features['activity_Q1'] / (features['activity_Q4'] + 1)\n",
    "    features['q4_ratio'] = features['activity_Q4'] / (features['total_events'] + 1)\n",
    "\n",
    "    print(\" Step 4/10: Page-level features...\")\n",
    "    page_counts = df_features.groupby(['userId', 'page']).size().unstack(fill_value=0)\n",
    "    page_totals = page_counts.sum(axis=1)\n",
    "    page_ratios = page_counts.div(page_totals, axis=0)\n",
    "    page_ratios.columns = [f'{col.lower().replace(\" \", \"_\")}_ratio' for col in page_ratios.columns]\n",
    "    features = features.merge(page_ratios, on='userId', how='left')\n",
    "    features = features.fillna(0)\n",
    "\n",
    "    print(\" Step 5/10: Downgrade detection...\")\n",
    "    df_sorted = df_features.sort_values(['userId', 'ts'])\n",
    "    df_sorted['prev_level'] = df_sorted.groupby('userId')['level'].shift(1)\n",
    "    df_sorted['downgrade'] = ((df_sorted['prev_level'] == 'paid') & (df_sorted['level'] == 'free')).astype(int)\n",
    "    has_downgraded = df_sorted.groupby('userId')['downgrade'].sum() > 0\n",
    "    features = features.merge(has_downgraded.astype(int).rename('has_downgraded'), on='userId', how='left')\n",
    "    features['has_downgraded'] = features['has_downgraded'].fillna(0)\n",
    "\n",
    "    paid_events = df_features[df_features['level'] == 'paid'].groupby('userId').size()\n",
    "    features = features.merge(paid_events.rename('paid_events'), on='userId', how='left')\n",
    "    features['paid_events'] = features['paid_events'].fillna(0)\n",
    "    features['paid_ratio'] = features['paid_events'] / (features['total_events'] + 1)\n",
    "\n",
    "    print(\" Step 6/10: Session features...\")\n",
    "    df_sorted = df_features.sort_values(['userId', 'ts'])\n",
    "    df_sorted['time_diff_minutes'] = df_sorted.groupby('userId')['ts'].diff().dt.total_seconds() / 60\n",
    "    df_sorted['new_session'] = (df_sorted['time_diff_minutes'] > 30) | df_sorted['time_diff_minutes'].isna()\n",
    "    df_sorted['session_id'] = df_sorted.groupby('userId')['new_session'].cumsum()\n",
    "\n",
    "    session_lengths = df_sorted.groupby(['userId', 'session_id']).size()\n",
    "    session_stats = session_lengths.groupby('userId').agg(['count', 'mean', 'std', 'max'])\n",
    "    session_stats.columns = ['num_sessions', 'avg_session_length', 'session_length_std', 'max_session_length']\n",
    "    features = features.merge(session_stats, on='userId', how='left')\n",
    "    features['sessions_per_day'] = features['num_sessions'] / total_days\n",
    "\n",
    "    print(\" Step 7/10: Engagement features...\")\n",
    "    positive_cols = ['thumbs_up_ratio', 'add_to_playlist_ratio', 'add_friend_ratio']\n",
    "    existing_positive = [col for col in positive_cols if col in features.columns]\n",
    "    features['positive_engagement'] = features[existing_positive].sum(axis=1) if existing_positive else 0\n",
    "\n",
    "    negative_cols = ['thumbs_down_ratio', 'error_ratio']\n",
    "    existing_negative = [col for col in negative_cols if col in features.columns]\n",
    "    features['negative_engagement'] = features[existing_negative].sum(axis=1) if existing_negative else 0\n",
    "\n",
    "    features['net_engagement'] = features['positive_engagement'] - features['negative_engagement']\n",
    "\n",
    "    print(\" Step 8/10: Time-of-day features...\")\n",
    "    weekend_events = df_features[df_features['is_weekend'] == 1].groupby('userId').size()\n",
    "    features = features.merge(weekend_events.rename('weekend_events'), on='userId', how='left')\n",
    "    features['weekend_events'] = features['weekend_events'].fillna(0)\n",
    "    features['weekend_ratio'] = features['weekend_events'] / (features['total_events'] + 1)\n",
    "\n",
    "    print(\" Step 9/10: Rolling window features...\")\n",
    "    last_7_days = df_features[df_features['ts'] >= (reference_ts - pd.Timedelta(days=7))]\n",
    "    last_7_counts = last_7_days.groupby('userId').size().rename('last_7_days_events')\n",
    "    features = features.merge(last_7_counts, on='userId', how='left')\n",
    "    features['last_7_days_events'] = features['last_7_days_events'].fillna(0)\n",
    "    features['last_7_days_ratio'] = features['last_7_days_events'] / (features['total_events'] + 1)\n",
    "\n",
    "    features['engagement_volatility'] = df_features.groupby('userId')['hour'].std()\n",
    "\n",
    "    unique_pages = df_features.groupby('userId')['page'].nunique()\n",
    "    features['page_diversity'] = unique_pages / (features['total_events'] + 1)\n",
    "\n",
    "    first_activity = df_features.groupby('userId')['ts'].min()\n",
    "    features['days_since_registration'] = (reference_ts - first_activity).dt.total_seconds() / 86400\n",
    "\n",
    "    last_3_days = df_features[df_features['ts'] >= (reference_ts - pd.Timedelta(days=3))]\n",
    "    prev_7_days = df_features[(df_features['ts'] >= (reference_ts - pd.Timedelta(days=10))) &\n",
    "                            (df_features['ts'] < (reference_ts - pd.Timedelta(days=3)))]\n",
    "\n",
    "    last_3_counts = last_3_days.groupby('userId').size() / 3\n",
    "    prev_7_counts = prev_7_days.groupby('userId').size() / 7\n",
    "    features['activity_acceleration'] = (last_3_counts - prev_7_counts) / (prev_7_counts + 1)\n",
    "\n",
    "    recent_errors = df_features[(df_features['ts'] >= (reference_ts - pd.Timedelta(days=7))) &\n",
    "                                (df_features['page'] == 'Error')]\n",
    "    features['recent_error_ratio'] = recent_errors.groupby('userId').size() / (features['last_7_days_events'] + 1)\n",
    "\n",
    "\n",
    "\n",
    "    print(\" Step 10/10: RFM score...\")\n",
    "    features['recency_percentile'] = features['recency_score'].rank(pct=True)\n",
    "    features['frequency_percentile'] = features['events_per_day'].rank(pct=True)\n",
    "    features['monetary_percentile'] = features['paid_ratio'].rank(pct=True)\n",
    "    features['rfm_score'] = (features['recency_percentile'] + features['frequency_percentile'] + features['monetary_percentile']) / 3\n",
    "\n",
    "    features = features.drop(columns=['last_ts', 'last_song_ts'], errors='ignore')\n",
    "    features = features.fillna(0)\n",
    "    features = features.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    print(f\" Created {len(features.columns)-1} features for {len(features):,} users\")\n",
    "\n",
    "    return features\n",
    "\n",
    "X_train_full = build_churn_features(train, is_train=True)\n",
    "X_test_full = build_churn_features(test, is_train=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Churn rate: 22.31%\n",
      "   Churners: 4,271\n",
      "   Non-churners: 14,869\n",
      "\n",
      " Final shapes:\n",
      "   X_train: (19140, 55)\n",
      "   X_test: (2904, 55)\n",
      "   Features: 55\n"
     ]
    }
   ],
   "source": [
    "churners = train[train['page'] == 'Cancellation Confirmation']['userId'].unique()\n",
    "y_train = X_train_full['userId'].isin(churners).astype(int)\n",
    "\n",
    "print(f\" Churn rate: {y_train.mean():.2%}\")\n",
    "print(f\"   Churners: {y_train.sum():,}\")\n",
    "print(f\"   Non-churners: {(len(y_train) - y_train.sum()):,}\")\n",
    "\n",
    "user_ids_train = X_train_full['userId']\n",
    "user_ids_test = X_test_full['userId']\n",
    "\n",
    "X_train = X_train_full.drop('userId', axis=1)\n",
    "X_test = X_test_full.drop('userId', axis=1)\n",
    "\n",
    "common_cols = X_train.columns.intersection(X_test.columns)\n",
    "X_train = X_train[common_cols]\n",
    "X_test = X_test[common_cols]\n",
    "\n",
    "print(f\"\\n Final shapes:\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   X_test: {X_test.shape}\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training XGBoost...\n",
      "   Fold 1/5... AUC: 0.7387\n",
      "   Fold 2/5... AUC: 0.7458\n",
      "   Fold 3/5... AUC: 0.7532\n",
      "   Fold 4/5... AUC: 0.7573\n",
      "   Fold 5/5... AUC: 0.7657\n",
      " XGBoost OOF AUC: 0.7519\n",
      "\n",
      " Training LightGBM...\n",
      "   Fold 1/5... Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[454]\tvalid_0's auc: 0.738398\n",
      "AUC: 0.7384\n",
      "   Fold 2/5... Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's auc: 0.748194\n",
      "AUC: 0.7482\n",
      "   Fold 3/5... Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[493]\tvalid_0's auc: 0.754066\n",
      "AUC: 0.7541\n",
      "   Fold 4/5... Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[491]\tvalid_0's auc: 0.760721\n",
      "AUC: 0.7607\n",
      "   Fold 5/5... Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[463]\tvalid_0's auc: 0.765479\n",
      "AUC: 0.7655\n",
      " LightGBM OOF AUC: 0.7532\n",
      " Ensemble OOF AUC: 0.7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    " #   X_train = X_train.drop('location', axis=1)\n",
    " #   X_test = X_test.drop('location', axis=1)\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"\\n Training XGBoost...\")\n",
    "xgb_params = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 500,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'scale_pos_weight': (len(y_train) - y_train.sum()) / y_train.sum(),\n",
    "    'random_state': 42,\n",
    "    'tree_method': 'hist'\n",
    "}\n",
    "\n",
    "xgb_oof = np.zeros(len(X_train))\n",
    "xgb_test = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(f\"   Fold {fold + 1}/{n_splits}...\", end=' ')\n",
    "\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "    xgb_oof[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    xgb_test += model.predict_proba(X_test)[:, 1] / n_splits\n",
    "\n",
    "    fold_auc = roc_auc_score(y_val, xgb_oof[val_idx])\n",
    "    print(f\"AUC: {fold_auc:.4f}\")\n",
    "\n",
    "xgb_auc = roc_auc_score(y_train, xgb_oof)\n",
    "print(f\" XGBoost OOF AUC: {xgb_auc:.4f}\")\n",
    "\n",
    "print(\"\\n Training LightGBM...\")\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': 6,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'scale_pos_weight': (len(y_train) - y_train.sum()) / y_train.sum(),\n",
    "    'n_estimators': 500,\n",
    "    'random_state': 42,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "lgb_oof = np.zeros(len(X_train))\n",
    "lgb_test = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(f\"   Fold {fold + 1}/{n_splits}...\", end=' ')\n",
    "\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n",
    "\n",
    "    lgb_oof[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    lgb_test += model.predict_proba(X_test)[:, 1] / n_splits\n",
    "\n",
    "    fold_auc = roc_auc_score(y_val, lgb_oof[val_idx])\n",
    "    print(f\"AUC: {fold_auc:.4f}\")\n",
    "\n",
    "lgb_auc = roc_auc_score(y_train, lgb_oof)\n",
    "print(f\" LightGBM OOF AUC: {lgb_auc:.4f}\")\n",
    "\n",
    "\n",
    "ensemble_oof = (xgb_oof + lgb_oof) / 2\n",
    "ensemble_test = (xgb_test + lgb_test) / 2\n",
    "\n",
    "ensemble_auc = roc_auc_score(y_train, ensemble_oof)\n",
    "print(f\" Ensemble OOF AUC: {ensemble_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Shape: (2904, 2)\n",
      "   Predicted churn rate: 34.33%\n",
      "   Predicted churners: 997\n",
      "   Predicted non-churners: 1,907\n",
      "   XGBoost AUC: 0.7519\n",
      "   LightGBM AUC: 0.7532\n",
      "   Ensemble AUC: 0.7532\n",
      "   Training churn rate: 22.31%\n",
      "        id  prediction\n",
      "0  1465194           1\n",
      "1  1261737           0\n",
      "2  1527155           0\n",
      "3  1507202           1\n",
      "4  1429412           0\n",
      "5  1778785           1\n",
      "6  1776591           1\n",
      "7  1937373           1\n",
      "8  1959334           1\n",
      "9  1138878           1\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "binary_predictions = (ensemble_test >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': user_ids_test,\n",
    "    'prediction': binary_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('churn_submission_4.csv', index=False)\n",
    "\n",
    "predicted_churn_pct = binary_predictions.mean() * 100\n",
    "\n",
    "print(f\"   Shape: {submission.shape}\")\n",
    "print(f\"   Predicted churn rate: {predicted_churn_pct:.2f}%\")\n",
    "print(f\"   Predicted churners: {binary_predictions.sum():,}\")\n",
    "print(f\"   Predicted non-churners: {(len(binary_predictions) - binary_predictions.sum()):,}\")\n",
    "\n",
    "print(f\"   XGBoost AUC: {xgb_auc:.4f}\")\n",
    "print(f\"   LightGBM AUC: {lgb_auc:.4f}\")\n",
    "print(f\"   Ensemble AUC: {ensemble_auc:.4f}\")\n",
    "print(f\"   Training churn rate: {y_train.mean()*100:.2f}%\")\n",
    "\n",
    "print(submission.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Shape: (2904, 2)\n",
      "   Predicted churn rate: 71.45%\n",
      "   Predicted churners: 2,075\n",
      "   Predicted non-churners: 829\n",
      "   XGBoost AUC: 0.7519\n",
      "   LightGBM AUC: 0.7532\n",
      "   Ensemble AUC: 0.7532\n",
      "   Training churn rate: 22.31%\n",
      "        id  prediction\n",
      "0  1465194           1\n",
      "1  1261737           1\n",
      "2  1527155           1\n",
      "3  1507202           1\n",
      "4  1429412           1\n",
      "5  1778785           1\n",
      "6  1776591           1\n",
      "7  1937373           1\n",
      "8  1959334           1\n",
      "9  1138878           1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "threshold = 0.3\n",
    "binary_predictions = (ensemble_test >= threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': user_ids_test,\n",
    "    'prediction': binary_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('churn_submission_4_0.3.csv', index=False)\n",
    "\n",
    "predicted_churn_pct = binary_predictions.mean() * 100\n",
    "\n",
    "print(f\"   Shape: {submission.shape}\")\n",
    "print(f\"   Predicted churn rate: {predicted_churn_pct:.2f}%\")\n",
    "print(f\"   Predicted churners: {binary_predictions.sum():,}\")\n",
    "print(f\"   Predicted non-churners: {(len(binary_predictions) - binary_predictions.sum()):,}\")\n",
    "\n",
    "\n",
    "print(f\"   XGBoost AUC: {xgb_auc:.4f}\")\n",
    "print(f\"   LightGBM AUC: {lgb_auc:.4f}\")\n",
    "print(f\"   Ensemble AUC: {ensemble_auc:.4f}\")\n",
    "print(f\"   Training churn rate: {y_train.mean()*100:.2f}%\")\n",
    "\n",
    "print(submission.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Kaggle : 0.628"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_arm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
